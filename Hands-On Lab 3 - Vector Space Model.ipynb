{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a0628e5",
   "metadata": {},
   "source": [
    "## Hands-On Lab 3 - Vector Space Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2802917",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9377a33",
   "metadata": {},
   "source": [
    "The *hotel reviews* data is stored as CSV file located within the *HotelReviews.zip* file. The *read_csv()* function from the *pandas* library will automatically load the CSV data from the ZIP file. Run the following code cell to load the data and display info about the data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de9cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hotel_reviews = pd.read_csv('HotelReviews.zip')\n",
    "hotel_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a290c545",
   "metadata": {},
   "source": [
    "### Step 2 - Custom Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4698203c",
   "metadata": {},
   "source": [
    "As discussed during lecture, the scikit-learn library classes provide extension points for using custom tokenization. The following code instantiates a global stopword list and Snowball stemmer so they are only created one time. Type the following code into the blank code cell in your lab notebook and run it to create the objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f531927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330b87ee",
   "metadata": {},
   "source": [
    "### Step 3 - Custom Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3098e090",
   "metadata": {},
   "source": [
    "The following function defines the NLTK-based custom tokenization and creates a document-term matrix using the custom tokenizer. Type the following code into the blank code cell in your lab notebook and run it to create the function and produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "310536f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7bda37d",
   "metadata": {},
   "source": [
    "### Step 4 - Adding N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92752a5f",
   "metadata": {},
   "source": [
    "The scikit-learn vectorizer classes can automatically create n-grams using the output of the custom tokenizer. The following code adds bigrams and trigrams to the document-term matrix. Type the following code into the blank code cell in your lab notebook and run it to create the function and produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c9e2e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b7a63",
   "metadata": {},
   "source": [
    "### Step 5 - Controlling Dimensionality Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90309e82",
   "metadata": {},
   "source": [
    "The current document-term matrix has 466,578 features! In most text analytics applications, the bulk of the cells in the matrix will be empty (i.e., they will have a count of zero). One way to control the dimensionality of the matrix is to specify the minimum number of documents that must include a term. The idea is that rarely occurring terms are not providing much information about the documents in the corpus. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a8c1cf01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07847b87",
   "metadata": {},
   "source": [
    "### Step 6 - Controlling Dimensionality Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ec8f0e",
   "metadata": {},
   "source": [
    "Another method of controlling dimensionality is to elminate terms that are incuded in too many documents. The idea is that very common terms are just noise in the corpus - much like stopwords.  Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d6fec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
