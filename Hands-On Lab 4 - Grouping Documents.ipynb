{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47866a3f",
   "metadata": {},
   "source": [
    "## Hands-On Lab 4 - Grouping Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a993",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530d558",
   "metadata": {},
   "source": [
    "The *hotel reviews* data is stored as CSV file located within the *HotelReviews.zip* file. The *read_csv()* function from the *pandas* library will automatically load the CSV data from the ZIP file. Run the following code cell to load the data and display info about the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hotel_reviews = pd.read_csv('HotelReviews.zip')\n",
    "hotel_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55442ee2",
   "metadata": {},
   "source": [
    "### Step 2 - Custom Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5b2f8",
   "metadata": {},
   "source": [
    "As discussed during lecture, the scikit-learn library classes provide extension points for using custom tokenization. The following code instantiates a global stopword list and Snowball stemmer so they are only created once. Run the following code to create the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7afbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Customize stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_words = [\"mustn't\", 'mustn', \"couldn't\", 'couldn', \"hadn't\", 'hadn', \n",
    "                \"didn't\", 'didn', \"wouldn't\", 'wouldn', \"wasn't\", 'wasn', \n",
    "                \"isn't\", 'isn', \"doesn't\", 'doesn', \"weren't\", 'weren', \n",
    "                \"hasn't\", \"hasn\"]\n",
    "\n",
    "for word in remove_words:\n",
    "    stop_words.remove(word)\n",
    "\n",
    "# Instantiate Snowball stemmer\n",
    "snowball_stemmer = SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbaa30f",
   "metadata": {},
   "source": [
    "### Step 3 - Custom Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19a6bb",
   "metadata": {},
   "source": [
    "Using the term frequency-inverse document frequency (TF-IDF) calculation has many benefits in text analytics. It is very commonly used in scenarios where documents are being grouped for smiliarity based on the vector space model. Run the following code to tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8672e549-696f-4f59-9d32-5c3f173c0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define custom tokenizer based on the NLTK\n",
    "def nltk_tokenizer(text):\n",
    "    raw_tokens = word_tokenize(text)\n",
    "    punctuation_tokens = [token for token in raw_tokens if not token in string.punctuation]\n",
    "    stop_words_tokens = [token for token in punctuation_tokens if not token in stop_words]\n",
    "    return([snowball_stemmer.stem(token) for token in stop_words_tokens])\n",
    "\n",
    "# Include bigrams/trigrams and constrain the dimensionality by requiring a term to show up \n",
    "# in at least 5 documents and less than 75% of all documents\n",
    "tfidf_vectorizer = TfidfVectorizer(tokenizer = nltk_tokenizer, token_pattern = None, \n",
    "                                   ngram_range = (1, 3), min_df = 5, max_df = 0.75)\n",
    "\n",
    "doc_term_matrix = tfidf_vectorizer.fit_transform(hotel_reviews['Review'])\n",
    "print(f'Rows: {doc_term_matrix.shape[0]}, Columns: {doc_term_matrix.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f9aea",
   "metadata": {},
   "source": [
    "### Step 4 - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f33a7",
   "metadata": {},
   "source": [
    "TF-IDF document-term matrices is a prime use case for cosine similarity. The TF-IDF calculation has a normalizing effect on the data (e.g., putting small documents on more equal footing with large documents) which often improves the cosine similarity results. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e612a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e986b9",
   "metadata": {},
   "source": [
    "### Step 5 - Clustering with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff7e0e",
   "metadata": {},
   "source": [
    "Another common use case for TD-IDF document-term matrices is clustering using the k-means algorithm. As you learned in lecture, k-means relies on distance for determining clusters and is sensitive to outliers. Again, TF-IDF's normalization of values can help make k-means more effective. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c34895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e0372",
   "metadata": {},
   "source": [
    "### Step 6 - Examining Clusters Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278c311",
   "metadata": {},
   "source": [
    "Looking at random samples of the documents assigned to each cluster is an important part of getting a feel for the effectiveness of the clustering. Larger random samples, of course, are more useful than smaller samples. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036be8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20da6c",
   "metadata": {},
   "source": [
    "### Step 7 - Examining Clusters Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7728bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
