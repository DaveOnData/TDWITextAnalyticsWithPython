{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47866a3f",
   "metadata": {},
   "source": [
    "## Hands-On Lab 4 - Grouping Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6a993",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8530d558",
   "metadata": {},
   "source": [
    "The *hotel reviews* data is stored as CSV file located within the *HotelReviews.zip* file. The *read_csv()* function from the *pandas* library will automatically load the CSV data from the ZIP file. Run the following code cell to load the data and display info about the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae43d1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hotel_reviews = pd.read_csv('HotelReviews.zip')\n",
    "hotel_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55442ee2",
   "metadata": {},
   "source": [
    "### Step 2 - Custom Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c5b2f8",
   "metadata": {},
   "source": [
    "As discussed during lecture, the scikit-learn library classes provide extension points for using custom tokenization. The following code instantiates a global stopword list and Snowball stemmer so they are only created one time. Type the following code into the blank code cell in your lab notebook and run it to create the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d7afbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbaa30f",
   "metadata": {},
   "source": [
    "### Step 3 - Custom Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a19a6bb",
   "metadata": {},
   "source": [
    "Using the term frequency-inverse document frequency (TF-IDF) calculation has many benefits in text analytics. It is very commonly used in scenarios where documents are being grouped for smiliarity based on the vector space model. Type the following code into the blank code cell in your lab notebook and run it to create the function and produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfe5a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007f9aea",
   "metadata": {},
   "source": [
    "### Step 4 - Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07f33a7",
   "metadata": {},
   "source": [
    "TF-IDF document-term matrices is a prime use case for cosine similarity. The TF-IDF calculation has a normalizing effect on the data (e.g., putting small documents on more equal footing with large documents) which often improves the cosine similarity results. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e612a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e986b9",
   "metadata": {},
   "source": [
    "### Step 5 - Clustering with K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feff7e0e",
   "metadata": {},
   "source": [
    "Another common use case for TD-IDF document-term matrices is clustering using the k-means algorithm. As you learned in lecture, k-means relies on distance for determining clusters and is sensitive to outliers. Again, TF-IDF's normalization of values can help make k-means more effective. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c34895",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567e0372",
   "metadata": {},
   "source": [
    "### Step 6 - Examining Clusters Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e278c311",
   "metadata": {},
   "source": [
    "Looking at random samples of the documents assigned to each cluster is an important part of getting a feel for the effectiveness of the clustering. Larger random samples, of course, are more useful than smaller samples. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "036be8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d20da6c",
   "metadata": {},
   "source": [
    "### Step 7 - Examining Clusters Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7728bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
