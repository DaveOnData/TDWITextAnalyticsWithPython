{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c151a1fe",
   "metadata": {},
   "source": [
    "## Hands-On Lab 5 - Document Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5348c60",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a4495a",
   "metadata": {},
   "source": [
    "The *hotel reviews* data is stored as CSV file located within the *HotelReviews.zip* file. The *read_csv()* function from the *pandas* library will automatically load the CSV data from the ZIP file. Run the following code cell to load the data and display info about the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307cc3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hotel_reviews = pd.read_csv('HotelReviews.zip')\n",
    "hotel_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91f916e",
   "metadata": {},
   "source": [
    "### Step 2 - Custom Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9134ae1",
   "metadata": {},
   "source": [
    "As discussed during lecture, the scikit-learn library classes provide extension points for using custom tokenization. The following code instantiates a global stopword list and Snowball stemmer so they are only created one time. Run the following code to create the objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df131d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Customize stopwords list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "remove_words = [\"mustn't\", 'mustn', \"couldn't\", 'couldn', \"hadn't\", 'hadn', \n",
    "                \"didn't\", 'didn', \"wouldn't\", 'wouldn', \"wasn't\", 'wasn', \n",
    "                \"isn't\", 'isn', \"doesn't\", 'doesn', \"weren't\", 'weren', \n",
    "                \"hasn't\", 'hasn', 'not']\n",
    "\n",
    "for word in remove_words:\n",
    "    stop_words.remove(word)\n",
    "\n",
    "# Instnatiate Snowball stemmer\n",
    "snowball_stemmer = SnowballStemmer(language = 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302a1462",
   "metadata": {},
   "source": [
    "### Step 3 - Custom Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1748d091",
   "metadata": {},
   "source": [
    "The following function defines the NLTK-based custom tokenization and creates a document-term matrix using the custom tokenizer. As discussed during lecture, the naive Bayes algorithm is typically used with token counts as opposed to TF-IDF. Run the follow code to tokenize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8fea94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define custom tokenizer based on the NLTK\n",
    "def nltk_tokenizer(text):\n",
    "    raw_tokens = word_tokenize(text)\n",
    "    punctuation_tokens = [token for token in raw_tokens if not token in string.punctuation]\n",
    "    stop_words_tokens = [token for token in punctuation_tokens if not token in stop_words]\n",
    "    return([snowball_stemmer.stem(token) for token in stop_words_tokens])\n",
    "\n",
    "# Add bigram/trigrams and constrain the dimensionality by requiring a term to show up in at \n",
    "# least 5 documents and less than 75% of all documents\n",
    "count_vectorizer = CountVectorizer(tokenizer = nltk_tokenizer, token_pattern = None,\n",
    "                                   ngram_range = (1, 3), min_df = 5, max_df = 0.75)\n",
    "\n",
    "doc_term_matrix = count_vectorizer.fit_transform(hotel_reviews['Review'])\n",
    "print(f'Rows: {doc_term_matrix.shape[0]}, Columns: {doc_term_matrix.shape[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55130209",
   "metadata": {},
   "source": [
    "### Step 4 - Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfb7af",
   "metadata": {},
   "source": [
    "As discussed in lecture, the naive Bayes algorithm is simple to understand, simple to train, and is surprisingly effective in classifying documents (e.g., email spam filtering). In this lab you will train a naive Bayes model to predict whether a reivew is positive (i.e., a rating of 4 or 5). Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "110cd449",
   "metadata": {},
   "source": [
    "### Step 5 - Tokenize the Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c3e335",
   "metadata": {},
   "source": [
    "Before predictions can be made, the test data needs to be tokenized in the same way the training data was tokenized. This ensures the vocabulary of the test data is the same as that of the training data. This is accomplished by reusing the CountVectorizer object. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a89e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4021c193",
   "metadata": {},
   "source": [
    "### Step 6 - Make Predictions and Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d2856cc",
   "metadata": {},
   "source": [
    "When classifying documents, you want some idea of how effective the predictive model will be when faced with new, unseen data. Using a test set allows you to simulate this scenario and estimate how effective the model will be. Type the following code into the blank code cell in your lab notebook and run it to produce the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c3d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your lab code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
